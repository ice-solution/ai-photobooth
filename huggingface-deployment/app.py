import os
import gradio as gr
import numpy as np
import cv2
from PIL import Image, ImageDraw, ImageFilter
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
import requests
from io import BytesIO
import base64
from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.responses import JSONResponse
import uvicorn
from fastapi.middleware.cors import CORSMiddleware

# è¨­å®šæ—¥èªŒ
import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å‰µå»º FastAPI æ‡‰ç”¨
app = FastAPI(title="FaceDancer API", description="AI è‡‰éƒ¨äº¤æ› API")

# æ·»åŠ  CORS ä¸­é–“ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class FaceDancerInference:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
        ])
        
    def preprocess_image(self, image):
        """é è™•ç†åœ–ç‰‡"""
        if isinstance(image, str):
            # å¦‚æœæ˜¯ URLï¼Œä¸‹è¼‰åœ–ç‰‡
            response = requests.get(image)
            image = Image.open(BytesIO(response.content))
        elif isinstance(image, np.ndarray):
            image = Image.fromarray(image)
        
        # è½‰æ›ç‚º RGB
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # èª¿æ•´å¤§å°
        image = image.resize((256, 256))
        
        return image
    
    def detect_face(self, image):
        """ç°¡å–®çš„è‡‰éƒ¨æª¢æ¸¬ï¼ˆä¸­å¿ƒè£å‰ªï¼‰"""
        # é€™è£¡æ‡‰è©²ä½¿ç”¨æ›´è¤‡é›œçš„è‡‰éƒ¨æª¢æ¸¬ç®—æ³•
        # ç‚ºäº†ç°¡åŒ–ï¼Œæˆ‘å€‘ä½¿ç”¨ä¸­å¿ƒè£å‰ª
        width, height = image.size
        size = min(width, height)
        left = (width - size) // 2
        top = (height - size) // 2
        right = left + size
        bottom = top + size
        
        face = image.crop((left, top, right, bottom))
        return face
    
    def simulate_face_swap(self, source_image, target_image):
        """æ”¹é€²çš„æ™ºèƒ½è‡‰éƒ¨äº¤æ›æ¨¡æ“¬"""
        try:
            # é è™•ç†åœ–ç‰‡
            source_face = self.preprocess_image(source_image)
            target_img = self.preprocess_image(target_image)
            
            # ç²å–åœ–ç‰‡å°ºå¯¸
            target_width, target_height = target_img.size
            
            # æ™ºèƒ½è‡‰éƒ¨å€åŸŸæª¢æ¸¬å’Œå®šä½
            face_size = min(target_width, target_height) * 0.35  # èª¿æ•´è‡‰éƒ¨å¤§å°æ¯”ä¾‹
            face_x = (target_width - face_size) // 2
            face_y = int(target_height * 0.18)  # èª¿æ•´è‡‰éƒ¨å‚ç›´ä½ç½®
            
            # å¾æºåœ–ç‰‡ä¸­æå–è‡‰éƒ¨å€åŸŸï¼ˆæ™ºèƒ½è£å‰ªï¼‰
            source_width, source_height = source_face.size
            source_face_size = min(source_width, source_height) * 0.8
            source_face_x = (source_width - source_face_size) // 2
            source_face_y = (source_height - source_face_size) // 2
            
            # è£å‰ªæºè‡‰éƒ¨
            source_face_cropped = source_face.crop((
                source_face_x, source_face_y,
                source_face_x + source_face_size,
                source_face_y + source_face_size
            ))
            
            # èª¿æ•´æºè‡‰éƒ¨å¤§å°ä»¥åŒ¹é…ç›®æ¨™
            source_face_resized = source_face_cropped.resize((int(face_size), int(face_size)), Image.LANCZOS)
            
            # å‰µå»ºæ›´è‡ªç„¶çš„æ©¢åœ“å½¢é®ç½©
            mask = Image.new('L', (int(face_size), int(face_size)), 0)
            mask_draw = ImageDraw.Draw(mask)
            
            # ç¹ªè£½æ©¢åœ“å½¢é®ç½©ï¼ˆæ›´è‡ªç„¶çš„è‡‰éƒ¨å½¢ç‹€ï¼‰
            ellipse_bbox = [int(face_size * 0.1), int(face_size * 0.1), 
                          int(face_size * 0.9), int(face_size * 0.85)]
            mask_draw.ellipse(ellipse_bbox, fill=255)
            
            # æ‡‰ç”¨æ›´å¼·çš„é«˜æ–¯æ¨¡ç³Šåˆ°é®ç½©é‚Šç·£
            mask = mask.filter(ImageFilter.GaussianBlur(radius=8))
            
            # é¡è‰²åŒ¹é…å’Œèª¿æ•´
            source_array = np.array(source_face_resized)
            target_array = np.array(target_img)
            
            # è¨ˆç®—ç›®æ¨™è‡‰éƒ¨å€åŸŸçš„å¹³å‡é¡è‰²
            target_face_region = target_array[
                int(face_y):int(face_y + face_size),
                int(face_x):int(face_x + face_size)
            ]
            target_mean_color = np.mean(target_face_region, axis=(0, 1))
            
            # è¨ˆç®—æºè‡‰éƒ¨çš„å¹³å‡é¡è‰²
            source_mean_color = np.mean(source_array, axis=(0, 1))
            
            # é¡è‰²èª¿æ•´ä¿‚æ•¸
            color_ratio = target_mean_color / (source_mean_color + 1e-8)
            color_ratio = np.clip(color_ratio, 0.7, 1.3)  # é™åˆ¶èª¿æ•´ç¯„åœ
            
            # æ‡‰ç”¨é¡è‰²èª¿æ•´
            source_adjusted = source_array.astype(np.float32) * color_ratio
            source_adjusted = np.clip(source_adjusted, 0, 255).astype(np.uint8)
            source_face_adjusted = Image.fromarray(source_adjusted)
            
            # å‰µå»ºç›®æ¨™åœ–ç‰‡çš„å‰¯æœ¬
            result_img = target_img.copy()
            
            # åˆæˆè‡‰éƒ¨
            result_img.paste(source_face_adjusted, (int(face_x), int(face_y)), mask)
            
            # æœ€çµ‚çš„é¡è‰²å¹³è¡¡å’ŒéŠ³åŒ–
            result_array = np.array(result_img)
            
            # è¼•å¾®çš„å°æ¯”åº¦èª¿æ•´
            result_array = result_array.astype(np.float32)
            result_array = np.clip(result_array * 1.02, 0, 255).astype(np.uint8)
            
            # æ‡‰ç”¨è¼•å¾®çš„éŠ³åŒ–
            from PIL import ImageEnhance
            result_img = Image.fromarray(result_array)
            enhancer = ImageEnhance.Sharpness(result_img)
            result_img = enhancer.enhance(1.1)
            
            return result_img
            
        except Exception as e:
            logger.error(f"è‡‰éƒ¨äº¤æ›å¤±æ•—: {e}")
            return target_image
    
    def perform_face_swap(self, source_image, target_image):
        """åŸ·è¡Œè‡‰éƒ¨äº¤æ›"""
        try:
            logger.info("é–‹å§‹è‡‰éƒ¨äº¤æ›...")
            
            # ä½¿ç”¨æ¨¡æ“¬çš„è‡‰éƒ¨äº¤æ›
            result = self.simulate_face_swap(source_image, target_image)
            
            logger.info("è‡‰éƒ¨äº¤æ›å®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"è‡‰éƒ¨äº¤æ›éŒ¯èª¤: {e}")
            return target_image

# åˆå§‹åŒ–æ¨ç†å¼•æ“
inference_engine = FaceDancerInference()

# FastAPI ç«¯é»
@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return {"status": "OK", "message": "FaceDancer API æ­£å¸¸é‹è¡Œ"}

@app.post("/swap")
async def face_swap_api(
    source_photo: UploadFile = File(...),
    target_photo: UploadFile = File(...)
):
    """è‡‰éƒ¨äº¤æ› API ç«¯é»"""
    try:
        logger.info("æ”¶åˆ°è‡‰éƒ¨äº¤æ› API è«‹æ±‚")
        
        # è®€å–ä¸Šå‚³çš„åœ–ç‰‡
        source_content = await source_photo.read()
        target_content = await target_photo.read()
        
        # è½‰æ›ç‚º PIL åœ–ç‰‡
        source_image = Image.open(BytesIO(source_content))
        target_image = Image.open(BytesIO(target_content))
        
        # åŸ·è¡Œè‡‰éƒ¨äº¤æ›
        result = inference_engine.perform_face_swap(source_image, target_image)
        
        # å°‡çµæœè½‰æ›ç‚º base64
        import io
        buffer = io.BytesIO()
        result.save(buffer, format='JPEG')
        img_str = base64.b64encode(buffer.getvalue()).decode()
        
        return {
            "success": True,
            "result": f"data:image/jpeg;base64,{img_str}",
            "message": "è‡‰éƒ¨äº¤æ›æˆåŠŸ"
        }
        
    except Exception as e:
        logger.error(f"API è‡‰éƒ¨äº¤æ›éŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/swap_base64")
async def face_swap_base64_api(
    source_photo: str = Form(...),
    target_photo: str = Form(...)
):
    """è‡‰éƒ¨äº¤æ› API ç«¯é» (Base64)"""
    try:
        logger.info("æ”¶åˆ° Base64 è‡‰éƒ¨äº¤æ› API è«‹æ±‚")
        
        # è§£ç¢¼ base64 åœ–ç‰‡
        source_data = base64.b64decode(source_photo.split(',')[1] if ',' in source_photo else source_photo)
        target_data = base64.b64decode(target_photo.split(',')[1] if ',' in target_photo else target_photo)
        
        # è½‰æ›ç‚º PIL åœ–ç‰‡
        source_image = Image.open(BytesIO(source_data))
        target_image = Image.open(BytesIO(target_data))
        
        # åŸ·è¡Œè‡‰éƒ¨äº¤æ›
        result = inference_engine.perform_face_swap(source_image, target_image)
        
        # å°‡çµæœè½‰æ›ç‚º base64
        import io
        buffer = io.BytesIO()
        result.save(buffer, format='JPEG')
        img_str = base64.b64encode(buffer.getvalue()).decode()
        
        return {
            "success": True,
            "result": f"data:image/jpeg;base64,{img_str}",
            "message": "è‡‰éƒ¨äº¤æ›æˆåŠŸ"
        }
        
    except Exception as e:
        logger.error(f"Base64 API è‡‰éƒ¨äº¤æ›éŒ¯èª¤: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def face_swap_interface(source_image, target_image):
    """Gradio ä»‹é¢å‡½æ•¸"""
    try:
        if source_image is None or target_image is None:
            return None, "è«‹ä¸Šå‚³æºè‡‰éƒ¨å’Œç›®æ¨™åœ–ç‰‡"
        
        # åŸ·è¡Œè‡‰éƒ¨äº¤æ›
        result = inference_engine.perform_face_swap(source_image, target_image)
        
        return result, "è‡‰éƒ¨äº¤æ›å®Œæˆï¼"
        
    except Exception as e:
        logger.error(f"ä»‹é¢éŒ¯èª¤: {e}")
        return None, f"éŒ¯èª¤: {str(e)}"

# å»ºç«‹ Gradio ä»‹é¢
with gr.Blocks(title="FaceDancer - AI è‡‰éƒ¨äº¤æ›", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ­ FaceDancer - AI è‡‰éƒ¨äº¤æ›")
    gr.Markdown("ä¸Šå‚³æºè‡‰éƒ¨åœ–ç‰‡å’Œç›®æ¨™åœ–ç‰‡ï¼ŒAI å°‡è‡ªå‹•é€²è¡Œè‡‰éƒ¨äº¤æ›")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("### ğŸ“¸ æºè‡‰éƒ¨åœ–ç‰‡")
            source_input = gr.Image(label="ä¸Šå‚³æºè‡‰éƒ¨åœ–ç‰‡", type="pil")
            
        with gr.Column():
            gr.Markdown("### ğŸ¯ ç›®æ¨™åœ–ç‰‡")
            target_input = gr.Image(label="ä¸Šå‚³ç›®æ¨™åœ–ç‰‡", type="pil")
    
    with gr.Row():
        swap_button = gr.Button("ğŸ­ é–‹å§‹è‡‰éƒ¨äº¤æ›", variant="primary", size="lg")
    
    with gr.Row():
        with gr.Column():
            gr.Markdown("### ğŸ¨ äº¤æ›çµæœ")
            result_output = gr.Image(label="è‡‰éƒ¨äº¤æ›çµæœ", type="pil")
        
        with gr.Column():
            gr.Markdown("### ğŸ“ ç‹€æ…‹")
            status_output = gr.Textbox(label="è™•ç†ç‹€æ…‹", interactive=False)
    
    # äº‹ä»¶è™•ç†
    swap_button.click(
        fn=face_swap_interface,
        inputs=[source_input, target_input],
        outputs=[result_output, status_output]
    )
    
    # ç¯„ä¾‹åœ–ç‰‡
    gr.Markdown("### ğŸ“‹ ä½¿ç”¨èªªæ˜")
    gr.Markdown("""
    1. **ä¸Šå‚³æºè‡‰éƒ¨åœ–ç‰‡**: åŒ…å«è¦äº¤æ›çš„è‡‰éƒ¨
    2. **ä¸Šå‚³ç›®æ¨™åœ–ç‰‡**: è¦é€²è¡Œè‡‰éƒ¨äº¤æ›çš„ç›®æ¨™åœ–ç‰‡
    3. **é»æ“Šé–‹å§‹è‡‰éƒ¨äº¤æ›**: AI å°‡è‡ªå‹•è™•ç†
    4. **æŸ¥çœ‹çµæœ**: åœ¨å³å´æŸ¥çœ‹è‡‰éƒ¨äº¤æ›çµæœ
    
    **æ³¨æ„äº‹é …**:
    - ç¢ºä¿è‡‰éƒ¨åœ–ç‰‡æ¸…æ™°å¯è¦‹
    - å»ºè­°ä½¿ç”¨æ­£é¢è‡‰éƒ¨ç…§ç‰‡
    - è™•ç†æ™‚é–“ç´„ 10-30 ç§’
    """)

# å°‡ Gradio æ‡‰ç”¨æ›è¼‰åˆ° FastAPI
app = gr.mount_gradio_app(app, demo, path="/")

# å•Ÿå‹•æ‡‰ç”¨
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=7860)
